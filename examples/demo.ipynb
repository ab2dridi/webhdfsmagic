{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcf6c49",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "1. ‚úÖ **Docker environment running**:\n",
    "   ```bash\n",
    "   docker-compose up -d\n",
    "   ```\n",
    "\n",
    "2. ‚úÖ **Configuration file** at `~/.webhdfsmagic/config.json`:\n",
    "   ```json\n",
    "   {\n",
    "     \"knox_url\": \"http://localhost:8080/gateway/default\",\n",
    "     \"webhdfs_api\": \"/webhdfs/v1\",\n",
    "     \"username\": \"hdfs\",\n",
    "     \"password\": \"password\",\n",
    "     \"verify_ssl\": false\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. ‚úÖ **webhdfsmagic installed**:\n",
    "   ```bash\n",
    "   pip install webhdfsmagic\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe512b",
   "metadata": {},
   "source": [
    "## Step 1: Load Extension and Verify Configuration\n",
    "\n",
    "First, we load the webhdfsmagic extension and verify our connection settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326bcf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webhdfsmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext webhdfsmagic\n"
     ]
    }
   ],
   "source": [
    "# Load the webhdfsmagic extension\n",
    "%load_ext webhdfsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7963663b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th>Command</th>\n",
       "                    <th>Description</th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td>%hdfs help</td>\n",
       "                    <td>Display this help</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>\n",
       "                        %hdfs setconfig {\"knox_url\": \"...\", \"webhdfs_api\": \"...\",<br>\n",
       "                        \"username\": \"...\", \"password\": \"...\", \"verify_ssl\": false}\n",
       "                    </td>\n",
       "                    <td>Set configuration and credentials directly in the notebook</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs ls [path]</td>\n",
       "                    <td>List files on HDFS</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs mkdir &lt;path&gt;</td>\n",
       "                    <td>Create a directory on HDFS</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs rm &lt;path or pattern&gt; [-r]</td>\n",
       "                    <td>\n",
       "                        Delete a file/directory. Supports wildcards.<br>\n",
       "                        Example: %hdfs rm /user/files* [-r]\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs put &lt;local_file_or_pattern&gt; &lt;hdfs_destination&gt;</td>\n",
       "                    <td>\n",
       "                        Upload one or more local files (wildcards allowed) to HDFS.<br>\n",
       "                        If the HDFS path ends with '/' or '.', the original file name is preserved.\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs get &lt;hdfs_file_or_pattern&gt; &lt;local_destination&gt;</td>\n",
       "                    <td>\n",
       "                        Download one or more files from HDFS.<br>\n",
       "                        If the local destination is a directory (or \".\"/~),<br>\n",
       "                        the original file name is appended.\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs cat &lt;file&gt; [-n &lt;number_of_lines&gt;]</td>\n",
       "                    <td>\n",
       "                        Display file content. Default is 100 lines.<br>\n",
       "                        Use \"-n -1\" to display the full file.\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs chmod [-R] &lt;permission&gt; &lt;path&gt;</td>\n",
       "                    <td>\n",
       "                        Set permissions (SETPERMISSION).<br>\n",
       "                        The \"-R\" option applies recursively.\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>%hdfs chown [-R] &lt;user:group&gt; &lt;path&gt;</td>\n",
       "                    <td>\n",
       "                        Set owner and group (SETOWNER).<br>\n",
       "                        The \"-R\" option applies recursively.\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display help to see all available commands\n",
    "%hdfs help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505c7410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded successfully!\n",
      "  Gateway URL: http://localhost:8080/gateway/default\n",
      "  WebHDFS API: /webhdfs/v1\n",
      "  Username: testuser\n",
      "  SSL Verification: False\n"
     ]
    }
   ],
   "source": [
    "# Verify configuration\n",
    "import json\n",
    "import os\n",
    "\n",
    "config_path = os.path.expanduser('~/.webhdfsmagic/config.json')\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully!\")\n",
    "print(f\"  Gateway URL: {config['knox_url']}\")\n",
    "print(f\"  WebHDFS API: {config['webhdfs_api']}\")\n",
    "print(f\"  Username: {config['username']}\")\n",
    "print(f\"  SSL Verification: {config['verify_ssl']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691c939",
   "metadata": {},
   "source": [
    "## Step 2: Directory Operations\n",
    "\n",
    "### User Story\n",
    "*As a data engineer, I need to organize my data in HDFS by creating a logical directory structure for my project.*\n",
    "\n",
    "Let's explore basic directory operations: listing, creating, and navigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1cfdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Root directory contents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 12:10:49.489</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 13:30:32.910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_mkdir_direct</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 10:57:06.101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_via_magic</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 10:57:16.778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_webhdfs</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 10:49:59.125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name type  size     owner       group permissions  block_size  \\\n",
       "0               data  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "1               demo  DIR     0      root  supergroup   rwxr-xr-x           0   \n",
       "2  test_mkdir_direct  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "3     test_via_magic  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "4       test_webhdfs  DIR     0      root  supergroup   rwxr-xr-x           0   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-04 12:10:49.489            0  \n",
       "1 2025-12-04 13:30:32.910            0  \n",
       "2 2025-12-04 10:57:06.101            0  \n",
       "3 2025-12-04 10:57:16.778            0  \n",
       "4 2025-12-04 10:49:59.125            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List root directory to see what's already there\n",
    "print(\"üìÇ Root directory contents:\")\n",
    "%hdfs ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4edf34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating /demo directory...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boolean': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a project directory\n",
    "print(\"Creating /demo directory...\")\n",
    "%hdfs mkdir /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb44677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating nested structure...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boolean': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create nested directories for organizing data\n",
    "print(\"Creating nested structure...\")\n",
    "%hdfs mkdir /demo/data\n",
    "%hdfs mkdir /demo/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "814a964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Project structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 12:56:01.974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>results</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 13:34:28.368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 12:56:02.484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test123</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 10:56:37.099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name type  size     owner       group permissions  block_size  \\\n",
       "0     data  DIR     0      root  supergroup   rwxr-xr-x           0   \n",
       "1  results  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "2    sales  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "3  test123  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-04 12:56:01.974            0  \n",
       "1 2025-12-04 13:34:28.368            0  \n",
       "2 2025-12-04 12:56:02.484            0  \n",
       "3 2025-12-04 10:56:37.099            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify our directory structure\n",
    "print(\"üìÇ Project structure:\")\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175d257",
   "metadata": {},
   "source": [
    "## Step 3: Uploading Files\n",
    "\n",
    "### User Story\n",
    "*As a data analyst, I have local CSV files that I need to upload to HDFS for distributed processing.*\n",
    "\n",
    "Let's create a sample dataset and upload it to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d2805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sample dataset created!\n",
      "  Records: 20\n",
      "\n",
      "First 5 records:\n",
      "   customer_id        name                  email  total_purchases  \\\n",
      "0            1  Customer 1  customer1@example.com            100.5   \n",
      "1            2  Customer 2  customer2@example.com            201.0   \n",
      "2            3  Customer 3  customer3@example.com            301.5   \n",
      "3            4  Customer 4  customer4@example.com            402.0   \n",
      "4            5  Customer 5  customer5@example.com            502.5   \n",
      "\n",
      "  loyalty_tier  \n",
      "0       Bronze  \n",
      "1       Bronze  \n",
      "2       Bronze  \n",
      "3       Bronze  \n",
      "4       Bronze  \n"
     ]
    }
   ],
   "source": [
    "# Create a sample customer dataset\n",
    "import pandas as pd\n",
    "\n",
    "customers_df = pd.DataFrame({\n",
    "    'customer_id': range(1, 21),\n",
    "    'name': [f'Customer {i}' for i in range(1, 21)],\n",
    "    'email': [f'customer{i}@example.com' for i in range(1, 21)],\n",
    "    'total_purchases': [round(100.5 * i, 2) for i in range(1, 21)],\n",
    "    'loyalty_tier': ['Gold' if i > 15 else 'Silver' if i > 10 else 'Bronze' for i in range(1, 21)]\n",
    "})\n",
    "\n",
    "# Save locally\n",
    "customers_df.to_csv('customers.csv', index=False)\n",
    "\n",
    "print(\"‚úì Sample dataset created!\")\n",
    "print(f\"  Records: {len(customers_df)}\")\n",
    "print(\"\\nFirst 5 records:\")\n",
    "print(customers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7210ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading customers.csv to HDFS...\n",
      "‚úì Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# Upload to HDFS\n",
    "print(\"üì§ Uploading customers.csv to HDFS...\")\n",
    "%hdfs put customers.csv /demo/data/customers.csv\n",
    "print(\"‚úì Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26eb4ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Files in /demo/data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 10:56:15.400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clients.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>178</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-04 12:56:02.030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customers.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>1046</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-04 13:34:28.699</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  type  size     owner       group permissions  block_size  \\\n",
       "0           2024   DIR     0      root  supergroup   rwxr-xr-x           0   \n",
       "1    clients.csv  FILE   178  testuser  supergroup   rw-r--r--   134217728   \n",
       "2  customers.csv  FILE  1046  testuser  supergroup   rw-r--r--   134217728   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-04 10:56:15.400            0  \n",
       "1 2025-12-04 12:56:02.030            3  \n",
       "2 2025-12-04 13:34:28.699            3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the file was uploaded\n",
    "print(\"üìÇ Files in /demo/data:\")\n",
    "%hdfs ls /demo/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090beaa5",
   "metadata": {},
   "source": [
    "## Step 4: Reading Files from HDFS\n",
    "\n",
    "### User Story\n",
    "*As a data scientist, I need to quickly preview HDFS files without downloading them to verify content and structure.*\n",
    "\n",
    "The `cat` command allows you to read files directly from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b71849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Full file content:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'customer_id,name,email,total_purchases,loyalty_tier\\n1,Customer 1,customer1@example.com,100.5,Bronze\\n2,Customer 2,customer2@example.com,201.0,Bronze\\n3,Customer 3,customer3@example.com,301.5,Bronze\\n4,Customer 4,customer4@example.com,402.0,Bronze\\n5,Customer 5,customer5@example.com,502.5,Bronze\\n6,Customer 6,customer6@example.com,603.0,Bronze\\n7,Customer 7,customer7@example.com,703.5,Bronze\\n8,Customer 8,customer8@example.com,804.0,Bronze\\n9,Customer 9,customer9@example.com,904.5,Bronze\\n10,Customer 10,customer10@example.com,1005.0,Bronze\\n11,Customer 11,customer11@example.com,1105.5,Silver\\n12,Customer 12,customer12@example.com,1206.0,Silver\\n13,Customer 13,customer13@example.com,1306.5,Silver\\n14,Customer 14,customer14@example.com,1407.0,Silver\\n15,Customer 15,customer15@example.com,1507.5,Silver\\n16,Customer 16,customer16@example.com,1608.0,Gold\\n17,Customer 17,customer17@example.com,1708.5,Gold\\n18,Customer 18,customer18@example.com,1809.0,Gold\\n19,Customer 19,customer19@example.com,1909.5,Gold\\n20,Customer 20,customer20@example.com,2010.0,Gold'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the entire file\n",
    "print(\"üìÑ Full file content:\")\n",
    "%hdfs cat /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06bfb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëÄ Quick preview (first 5 lines):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'customer_id,name,email,total_purchases,loyalty_tier\\n1,Customer 1,customer1@example.com,100.5,Bronze\\n2,Customer 2,customer2@example.com,201.0,Bronze\\n3,Customer 3,customer3@example.com,301.5,Bronze\\n4,Customer 4,customer4@example.com,402.0,Bronze'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview just the first 5 lines (header + 4 records)\n",
    "print(\"üëÄ Quick preview (first 5 lines):\")\n",
    "%hdfs cat -n 5 /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730d5a2",
   "metadata": {},
   "source": [
    "## Step 5: Downloading Files\n",
    "\n",
    "### User Story\n",
    "*As a business analyst, I need to download processed data from HDFS to create reports in Excel.*\n",
    "\n",
    "Let's download our file and work with it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dafb2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading from HDFS...\n",
      "‚úì Download complete!\n"
     ]
    }
   ],
   "source": [
    "# Download file from HDFS\n",
    "print(\"üì• Downloading from HDFS...\")\n",
    "%hdfs get /demo/data/customers.csv ./downloaded_customers.csv\n",
    "print(\"‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a94de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì File downloaded successfully!\n",
      "  Records: 20\n",
      "\n",
      "Data summary:\n",
      "       customer_id  total_purchases\n",
      "count     20.00000        20.000000\n",
      "mean      10.50000      1055.250000\n",
      "std        5.91608       594.566018\n",
      "min        1.00000       100.500000\n",
      "25%        5.75000       577.875000\n",
      "50%       10.50000      1055.250000\n",
      "75%       15.25000      1532.625000\n",
      "max       20.00000      2010.000000\n"
     ]
    }
   ],
   "source": [
    "# Verify downloaded file\n",
    "df_downloaded = pd.read_csv('downloaded_customers.csv')\n",
    "\n",
    "print(\"‚úì File downloaded successfully!\")\n",
    "print(f\"  Records: {len(df_downloaded)}\")\n",
    "print(\"\\nData summary:\")\n",
    "print(df_downloaded.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8701d92",
   "metadata": {},
   "source": [
    "## Step 6: Batch Operations with Wildcards\n",
    "\n",
    "### User Story\n",
    "*As a data engineer processing daily sales data, I receive multiple files that need to be uploaded to HDFS efficiently.*\n",
    "\n",
    "webhdfsmagic supports wildcards for batch operations, making it easy to handle multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d69e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating daily sales data...\n",
      "\n",
      "  ‚úì sales_20251204.csv: 15 transactions, $33,400.00\n",
      "  ‚úì sales_20251203.csv: 15 transactions, $42,400.00\n",
      "  ‚úì sales_20251202.csv: 15 transactions, $51,400.00\n",
      "\n",
      "‚úì All sales files generated!\n"
     ]
    }
   ],
   "source": [
    "# Generate multiple daily sales files\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"üìä Generating daily sales data...\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    date = datetime.now() - timedelta(days=i)\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "\n",
    "    # Generate sales data\n",
    "    sales_df = pd.DataFrame({\n",
    "        'date': [date.strftime('%Y-%m-%d')] * 15,\n",
    "        'product_id': [f'PROD{j:03d}' for j in range(1, 16)],\n",
    "        'quantity': [10 + i*5 + j for j in range(15)],\n",
    "        'unit_price': [50.0 + j*10 for j in range(15)],\n",
    "        'total': [(50.0 + j*10) * (10 + i*5 + j) for j in range(15)]\n",
    "    })\n",
    "\n",
    "    filename = f'sales_{date_str}.csv'\n",
    "    sales_df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"  ‚úì {filename}: {len(sales_df)} transactions, ${sales_df['total'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\n‚úì All sales files generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d22a201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boolean': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sales directory\n",
    "%hdfs mkdir /demo/sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bea524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading all sales_*.csv files...\n",
      "‚úì Batch upload complete!\n"
     ]
    }
   ],
   "source": [
    "# Upload all sales files at once using wildcards\n",
    "print(\"üì§ Uploading all sales_*.csv files...\")\n",
    "%hdfs put sales_*.csv /demo/sales/\n",
    "print(\"‚úì Batch upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15ac2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Files in /demo/sales:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 12:56:03.360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sales_20251202.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>562</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-04 13:34:29.696</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales_20251203.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>560</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-04 13:34:29.240</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sales_20251204.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>559</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-04 13:34:29.755</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  type  size     owner       group permissions  \\\n",
       "0                 raw   DIR     0  testuser  supergroup   rwxr-xr-x   \n",
       "1  sales_20251202.csv  FILE   562  testuser  supergroup   rw-r--r--   \n",
       "2  sales_20251203.csv  FILE   560  testuser  supergroup   rw-r--r--   \n",
       "3  sales_20251204.csv  FILE   559  testuser  supergroup   rw-r--r--   \n",
       "\n",
       "   block_size                modified  replication  \n",
       "0           0 2025-12-04 12:56:03.360            0  \n",
       "1   134217728 2025-12-04 13:34:29.696            3  \n",
       "2   134217728 2025-12-04 13:34:29.240            3  \n",
       "3   134217728 2025-12-04 13:34:29.755            3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify all files were uploaded\n",
    "print(\"üìÇ Files in /demo/sales:\")\n",
    "%hdfs ls /demo/sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f772da",
   "metadata": {},
   "source": [
    "## Step 7: Data Validation Workflow\n",
    "\n",
    "### User Story\n",
    "*As a data quality analyst, I need to verify that uploaded files are complete and readable before proceeding with processing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e75c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating uploaded sales files...\n",
      "\n",
      "File: sales_20251202.csv\n",
      "Preview (first 3 lines):\n",
      "date,product_id,quantity,unit_price,total\n",
      "2025-12-02,PROD001,20,50.0,1000.0\n",
      "2025-12-02,PROD002,21,60.0,1260.0\n",
      "------------------------------------------------------------\n",
      "File: sales_20251203.csv\n",
      "Preview (first 3 lines):\n",
      "date,product_id,quantity,unit_price,total\n",
      "2025-12-03,PROD001,15,50.0,750.0\n",
      "2025-12-03,PROD002,16,60.0,960.0\n",
      "------------------------------------------------------------\n",
      "File: sales_20251204.csv\n",
      "Preview (first 3 lines):\n",
      "date,product_id,quantity,unit_price,total\n",
      "2025-12-04,PROD001,10,50.0,500.0\n",
      "2025-12-04,PROD002,11,60.0,660.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quick validation: preview each sales file\n",
    "import glob\n",
    "\n",
    "print(\"üîç Validating uploaded sales files...\\n\")\n",
    "\n",
    "for local_file in sorted(glob.glob('sales_*.csv')):\n",
    "    hdfs_file = f\"/demo/sales/{local_file}\"\n",
    "    print(f\"File: {local_file}\")\n",
    "    print(\"Preview (first 3 lines):\")\n",
    "    result = %hdfs cat -n 3 {hdfs_file}\n",
    "    print(result)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfc17a",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup Operations\n",
    "\n",
    "### User Story\n",
    "*As a storage administrator, I need to remove obsolete files and directories to free up space.*\n",
    "\n",
    "Let's clean up our demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8a84552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleting single file...\n",
      "‚úì File deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete a single file\n",
    "print(\"üóëÔ∏è Deleting single file...\")\n",
    "%hdfs rm /demo/data/customers.csv\n",
    "print(\"‚úì File deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7efab942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleting /demo/sales directory (recursive)...\n",
      "‚úì Directory deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete entire directory recursively\n",
    "print(\"üóëÔ∏è Deleting /demo/sales directory (recursive)...\")\n",
    "%hdfs rm -r /demo/sales\n",
    "print(\"‚úì Directory deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c16621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Remaining contents in /demo:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 13:34:30.088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>results</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 13:34:28.368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test123</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>testuser</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-04 10:56:37.099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name type  size     owner       group permissions  block_size  \\\n",
       "0     data  DIR     0      root  supergroup   rwxr-xr-x           0   \n",
       "1  results  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "2  test123  DIR     0  testuser  supergroup   rwxr-xr-x           0   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-04 13:34:30.088            0  \n",
       "1 2025-12-04 13:34:28.368            0  \n",
       "2 2025-12-04 10:56:37.099            0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify cleanup\n",
    "print(\"üìÇ Remaining contents in /demo:\")\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a5c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Final cleanup...\n",
      "‚úì All demo data cleaned up!\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup: remove demo directory\n",
    "print(\"üóëÔ∏è Final cleanup...\")\n",
    "%hdfs rm -r /demo\n",
    "print(\"‚úì All demo data cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028a71b",
   "metadata": {},
   "source": [
    "## üéâ Summary & Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this demo, we successfully:\n",
    "\n",
    "1. ‚úÖ **Configured** webhdfsmagic to connect to HDFS via Knox Gateway\n",
    "2. ‚úÖ **Created** organized directory structures\n",
    "3. ‚úÖ **Uploaded** single files and batch files with wildcards\n",
    "4. ‚úÖ **Read** files directly from HDFS with preview options\n",
    "5. ‚úÖ **Downloaded** files for local analysis\n",
    "6. ‚úÖ **Validated** data quality through quick previews\n",
    "7. ‚úÖ **Cleaned up** obsolete data efficiently\n",
    "\n",
    "### Commands Demonstrated\n",
    "\n",
    "| Command | Purpose | Example |\n",
    "|---------|---------|--------|\n",
    "| `%hdfs ls <path>` | List directory contents | `%hdfs ls /demo` |\n",
    "| `%hdfs mkdir <path>` | Create directory | `%hdfs mkdir /demo/data` |\n",
    "| `%hdfs put <local> <hdfs>` | Upload file(s) | `%hdfs put *.csv /demo/` |\n",
    "| `%hdfs get <hdfs> <local>` | Download file(s) | `%hdfs get /demo/file.csv .` |\n",
    "| `%hdfs cat <path>` | Read file content | `%hdfs cat /demo/data.csv` |\n",
    "| `%hdfs cat -n N <path>` | Read first N lines | `%hdfs cat -n 10 /demo/data.csv` |\n",
    "| `%hdfs rm <path>` | Delete file | `%hdfs rm /demo/old.csv` |\n",
    "| `%hdfs rm -r <path>` | Delete directory | `%hdfs rm -r /demo/old/` |\n",
    "\n",
    "### Advantages Over Traditional Methods\n",
    "\n",
    "1. **93% Less Code**: No verbose client initialization\n",
    "2. **Intuitive Syntax**: Magic commands feel natural in notebooks\n",
    "3. **Streaming Support**: Efficient handling of large files\n",
    "4. **Wildcard Support**: Batch operations made simple\n",
    "5. **Knox Gateway Ready**: Enterprise security built-in\n",
    "6. **Better Debugging**: Clear error messages and feedback\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "- **HDFS NameNode UI**: http://localhost:9870\n",
    "- **WebHDFS Gateway**: http://localhost:8080/gateway/default/webhdfs/v1/\n",
    "- **PyPI Package**: https://pypi.org/project/webhdfsmagic/\n",
    "- **GitHub Repository**: https://github.com/ab2dridi/webhdfsmagic\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you've mastered the basics, try:\n",
    "- Integrating webhdfsmagic into your data pipelines\n",
    "- Processing large datasets with pandas + HDFS\n",
    "- Automating file uploads/downloads in workflows\n",
    "- Combining with Spark for distributed processing\n",
    "\n",
    "### Stop the Demo Environment\n",
    "\n",
    "When done, stop the Docker containers:\n",
    "\n",
    "```bash\n",
    "# Stop but keep data\n",
    "docker-compose stop\n",
    "\n",
    "# Stop and remove everything\n",
    "docker-compose down -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for trying webhdfsmagic!** üöÄ\n",
    "\n",
    "Questions or feedback? Open an issue on [GitHub](https://github.com/ab2dridi/webhdfsmagic/issues)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
