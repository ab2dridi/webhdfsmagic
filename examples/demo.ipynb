{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcf6c49",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "1. ‚úÖ **Docker environment running**:\n",
    "   ```bash\n",
    "   docker-compose up -d\n",
    "   ```\n",
    "\n",
    "2. ‚úÖ **Configuration file** at `~/.webhdfsmagic/config.json`:\n",
    "   ```json\n",
    "   {\n",
    "     \"knox_url\": \"http://localhost:8080/gateway/default\",\n",
    "     \"webhdfs_api\": \"/webhdfs/v1\",\n",
    "     \"username\": \"hdfs\",\n",
    "     \"password\": \"password\",\n",
    "     \"verify_ssl\": false\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. ‚úÖ **webhdfsmagic installed**:\n",
    "   ```bash\n",
    "   pip install webhdfsmagic\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe512b",
   "metadata": {},
   "source": [
    "## Step 1: Load Extension and Verify Configuration\n",
    "\n",
    "First, we load the webhdfsmagic extension and verify our connection settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bcf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the webhdfsmagic extension\n",
    "%load_ext webhdfsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display help to see all available commands\n",
    "%hdfs help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify configuration\n",
    "import json\n",
    "import os\n",
    "\n",
    "config_path = os.path.expanduser('~/.webhdfsmagic/config.json')\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "print(\"‚úì Configuration loaded successfully!\")\n",
    "print(f\"  Gateway URL: {config['knox_url']}\")\n",
    "print(f\"  WebHDFS API: {config['webhdfs_api']}\")\n",
    "print(f\"  Username: {config['username']}\")\n",
    "print(f\"  SSL Verification: {config['verify_ssl']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691c939",
   "metadata": {},
   "source": [
    "## Step 2: Directory Operations\n",
    "\n",
    "### User Story\n",
    "*As a data engineer, I need to organize my data in HDFS by creating a logical directory structure for my project.*\n",
    "\n",
    "Let's explore basic directory operations: listing, creating, and navigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cfdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List root directory to see what's already there\n",
    "print(\"üìÇ Root directory contents:\")\n",
    "%hdfs ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project directory\n",
    "print(\"Creating /demo directory...\")\n",
    "%hdfs mkdir /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb44677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested directories for organizing data\n",
    "print(\"Creating nested structure...\")\n",
    "%hdfs mkdir /demo/data\n",
    "%hdfs mkdir /demo/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our directory structure\n",
    "print(\"üìÇ Project structure:\")\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175d257",
   "metadata": {},
   "source": [
    "## Step 3: Uploading Files\n",
    "\n",
    "### User Story\n",
    "*As a data analyst, I have local CSV files that I need to upload to HDFS for distributed processing.*\n",
    "\n",
    "Let's create a sample dataset and upload it to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample customer dataset\n",
    "import pandas as pd\n",
    "\n",
    "customers_df = pd.DataFrame({\n",
    "    'customer_id': range(1, 21),\n",
    "    'name': [f'Customer {i}' for i in range(1, 21)],\n",
    "    'email': [f'customer{i}@example.com' for i in range(1, 21)],\n",
    "    'total_purchases': [round(100.5 * i, 2) for i in range(1, 21)],\n",
    "    'loyalty_tier': ['Gold' if i > 15 else 'Silver' if i > 10 else 'Bronze' for i in range(1, 21)]\n",
    "})\n",
    "\n",
    "# Save locally\n",
    "customers_df.to_csv('customers.csv', index=False)\n",
    "\n",
    "print(\"‚úì Sample dataset created!\")\n",
    "print(f\"  Records: {len(customers_df)}\")\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "print(customers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7210ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HDFS\n",
    "print(\"üì§ Uploading customers.csv to HDFS...\")\n",
    "%hdfs put customers.csv /demo/data/customers.csv\n",
    "print(\"‚úì Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file was uploaded\n",
    "print(\"üìÇ Files in /demo/data:\")\n",
    "%hdfs ls /demo/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090beaa5",
   "metadata": {},
   "source": [
    "## Step 4: Reading Files from HDFS\n",
    "\n",
    "### User Story\n",
    "*As a data scientist, I need to quickly preview HDFS files without downloading them to verify content and structure.*\n",
    "\n",
    "The `cat` command allows you to read files directly from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b71849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire file\n",
    "print(\"üìÑ Full file content:\")\n",
    "%hdfs cat /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview just the first 5 lines (header + 4 records)\n",
    "print(\"üëÄ Quick preview (first 5 lines):\")\n",
    "%hdfs cat -n 5 /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730d5a2",
   "metadata": {},
   "source": [
    "## Step 5: Downloading Files\n",
    "\n",
    "### User Story\n",
    "*As a business analyst, I need to download processed data from HDFS to create reports in Excel.*\n",
    "\n",
    "Let's download our file and work with it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from HDFS\n",
    "print(\"üì• Downloading from HDFS...\")\n",
    "%hdfs get /demo/data/customers.csv ./downloaded_customers.csv\n",
    "print(\"‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify downloaded file\n",
    "df_downloaded = pd.read_csv('downloaded_customers.csv')\n",
    "\n",
    "print(\"‚úì File downloaded successfully!\")\n",
    "print(f\"  Records: {len(df_downloaded)}\")\n",
    "print(f\"\\nData summary:\")\n",
    "print(df_downloaded.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8701d92",
   "metadata": {},
   "source": [
    "## Step 6: Batch Operations with Wildcards\n",
    "\n",
    "### User Story\n",
    "*As a data engineer processing daily sales data, I receive multiple files that need to be uploaded to HDFS efficiently.*\n",
    "\n",
    "webhdfsmagic supports wildcards for batch operations, making it easy to handle multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple daily sales files\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"üìä Generating daily sales data...\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    date = datetime.now() - timedelta(days=i)\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Generate sales data\n",
    "    sales_df = pd.DataFrame({\n",
    "        'date': [date.strftime('%Y-%m-%d')] * 15,\n",
    "        'product_id': [f'PROD{j:03d}' for j in range(1, 16)],\n",
    "        'quantity': [10 + i*5 + j for j in range(15)],\n",
    "        'unit_price': [50.0 + j*10 for j in range(15)],\n",
    "        'total': [(50.0 + j*10) * (10 + i*5 + j) for j in range(15)]\n",
    "    })\n",
    "    \n",
    "    filename = f'sales_{date_str}.csv'\n",
    "    sales_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"  ‚úì {filename}: {len(sales_df)} transactions, ${sales_df['total'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\n‚úì All sales files generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales directory\n",
    "%hdfs mkdir /demo/sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload all sales files at once using wildcards\n",
    "print(\"üì§ Uploading all sales_*.csv files...\")\n",
    "%hdfs put sales_*.csv /demo/sales/\n",
    "print(\"‚úì Batch upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all files were uploaded\n",
    "print(\"üìÇ Files in /demo/sales:\")\n",
    "%hdfs ls /demo/sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f772da",
   "metadata": {},
   "source": [
    "## Step 7: Data Validation Workflow\n",
    "\n",
    "### User Story\n",
    "*As a data quality analyst, I need to verify that uploaded files are complete and readable before proceeding with processing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e75c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation: preview each sales file\n",
    "import glob\n",
    "\n",
    "print(\"üîç Validating uploaded sales files...\\n\")\n",
    "\n",
    "for local_file in sorted(glob.glob('sales_*.csv')):\n",
    "    hdfs_file = f\"/demo/sales/{local_file}\"\n",
    "    print(f\"File: {local_file}\")\n",
    "    print(f\"Preview (first 3 lines):\")\n",
    "    result = %hdfs cat -n 3 {hdfs_file}\n",
    "    print(result)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfc17a",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup Operations\n",
    "\n",
    "### User Story\n",
    "*As a storage administrator, I need to remove obsolete files and directories to free up space.*\n",
    "\n",
    "Let's clean up our demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a84552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a single file\n",
    "print(\"üóëÔ∏è Deleting single file...\")\n",
    "%hdfs rm /demo/data/customers.csv\n",
    "print(\"‚úì File deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efab942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete entire directory recursively\n",
    "print(\"üóëÔ∏è Deleting /demo/sales directory (recursive)...\")\n",
    "%hdfs rm -r /demo/sales\n",
    "print(\"‚úì Directory deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c16621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleanup\n",
    "print(\"üìÇ Remaining contents in /demo:\")\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup: remove demo directory\n",
    "print(\"üóëÔ∏è Final cleanup...\")\n",
    "%hdfs rm -r /demo\n",
    "print(\"‚úì All demo data cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028a71b",
   "metadata": {},
   "source": [
    "## üéâ Summary & Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this demo, we successfully:\n",
    "\n",
    "1. ‚úÖ **Configured** webhdfsmagic to connect to HDFS via Knox Gateway\n",
    "2. ‚úÖ **Created** organized directory structures\n",
    "3. ‚úÖ **Uploaded** single files and batch files with wildcards\n",
    "4. ‚úÖ **Read** files directly from HDFS with preview options\n",
    "5. ‚úÖ **Downloaded** files for local analysis\n",
    "6. ‚úÖ **Validated** data quality through quick previews\n",
    "7. ‚úÖ **Cleaned up** obsolete data efficiently\n",
    "\n",
    "### Commands Demonstrated\n",
    "\n",
    "| Command | Purpose | Example |\n",
    "|---------|---------|--------|\n",
    "| `%hdfs ls <path>` | List directory contents | `%hdfs ls /demo` |\n",
    "| `%hdfs mkdir <path>` | Create directory | `%hdfs mkdir /demo/data` |\n",
    "| `%hdfs put <local> <hdfs>` | Upload file(s) | `%hdfs put *.csv /demo/` |\n",
    "| `%hdfs get <hdfs> <local>` | Download file(s) | `%hdfs get /demo/file.csv .` |\n",
    "| `%hdfs cat <path>` | Read file content | `%hdfs cat /demo/data.csv` |\n",
    "| `%hdfs cat -n N <path>` | Read first N lines | `%hdfs cat -n 10 /demo/data.csv` |\n",
    "| `%hdfs rm <path>` | Delete file | `%hdfs rm /demo/old.csv` |\n",
    "| `%hdfs rm -r <path>` | Delete directory | `%hdfs rm -r /demo/old/` |\n",
    "\n",
    "### Advantages Over Traditional Methods\n",
    "\n",
    "1. **93% Less Code**: No verbose client initialization\n",
    "2. **Intuitive Syntax**: Magic commands feel natural in notebooks\n",
    "3. **Streaming Support**: Efficient handling of large files\n",
    "4. **Wildcard Support**: Batch operations made simple\n",
    "5. **Knox Gateway Ready**: Enterprise security built-in\n",
    "6. **Better Debugging**: Clear error messages and feedback\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "- **HDFS NameNode UI**: http://localhost:9870\n",
    "- **WebHDFS Gateway**: http://localhost:8080/gateway/default/webhdfs/v1/\n",
    "- **PyPI Package**: https://pypi.org/project/webhdfsmagic/\n",
    "- **GitHub Repository**: https://github.com/ab2dridi/webhdfsmagic\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you've mastered the basics, try:\n",
    "- Integrating webhdfsmagic into your data pipelines\n",
    "- Processing large datasets with pandas + HDFS\n",
    "- Automating file uploads/downloads in workflows\n",
    "- Combining with Spark for distributed processing\n",
    "\n",
    "### Stop the Demo Environment\n",
    "\n",
    "When done, stop the Docker containers:\n",
    "\n",
    "```bash\n",
    "# Stop but keep data\n",
    "docker-compose stop\n",
    "\n",
    "# Stop and remove everything\n",
    "docker-compose down -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for trying webhdfsmagic!** üöÄ\n",
    "\n",
    "Questions or feedback? Open an issue on [GitHub](https://github.com/ab2dridi/webhdfsmagic/issues)!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
