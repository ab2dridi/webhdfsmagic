{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e66f39a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running this demo, ensure you have:\n",
    "1. Docker and docker-compose installed\n",
    "2. Started the HDFS environment: `docker-compose up -d`\n",
    "3. Configuration file at `~/.webhdfsmagic/config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extension\n",
    "%load_ext webhdfsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d669ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View help and available commands\n",
    "%hdfs help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56787a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current configuration\n",
    "import json\n",
    "import os\n",
    "\n",
    "config_path = os.path.expanduser('~/.webhdfsmagic/config.json')\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "print(\"Current configuration:\")\n",
    "print(f\"  URL: {config['knox_url']}{config['webhdfs_api']}\")\n",
    "print(f\"  User: {config['username']}\")\n",
    "print(f\"  SSL: {config['verify_ssl']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ebf97",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb699603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List root directory\n",
    "%hdfs ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949dc04d",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Creating Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test directory\n",
    "%hdfs mkdir /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b746604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested directories\n",
    "%hdfs mkdir /demo/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3eb397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify directory creation\n",
    "%hdfs ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069013ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents of demo directory\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2302c",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Uploading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b769bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local test file\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'customer': [f'Customer{i}' for i in range(1, 11)],\n",
    "    'amount': [100.5 * i for i in range(1, 11)]\n",
    "})\n",
    "\n",
    "# Save locally\n",
    "df.to_csv('test_data.csv', index=False)\n",
    "print(\"File test_data.csv created:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HDFS\n",
    "%hdfs put test_data.csv /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify file exists\n",
    "%hdfs ls /demo/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b5dea",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file content\n",
    "%hdfs cat /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dda5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only first 5 lines\n",
    "%hdfs cat -n 5 /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8b78b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Downloading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from HDFS\n",
    "%hdfs get /demo/data/customers.csv ./downloaded_customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify downloaded file\n",
    "df_downloaded = pd.read_csv('downloaded_customers.csv')\n",
    "print(\"File downloaded from HDFS:\")\n",
    "print(df_downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3528cc1",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Complete Workflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ea303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple sales data files\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"üìä Generating sales data...\")\n",
    "\n",
    "for i in range(3):\n",
    "    date = datetime.now() - timedelta(days=i)\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "    \n",
    "    # Generate data\n",
    "    df_sales = pd.DataFrame({\n",
    "        'date': [date.strftime('%Y-%m-%d')] * 10,\n",
    "        'product_id': range(1, 11),\n",
    "        'quantity': [10 + i*5 + j for j in range(10)],\n",
    "        'price': [50.0 + j*10 for j in range(10)]\n",
    "    })\n",
    "    \n",
    "    filename = f'sales_{date_str}.csv'\n",
    "    df_sales.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"  Created: {filename} ({len(df_sales)} rows)\")\n",
    "\n",
    "print(\"\\n‚úì Data generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7baf225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create destination directory\n",
    "%hdfs mkdir /demo/sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload all files using wildcards\n",
    "%hdfs put sales_*.csv /demo/sales/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86513a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploaded files\n",
    "print(\"üìÅ Files in HDFS:\\n\")\n",
    "%hdfs ls /demo/sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e3452",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31809d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a file\n",
    "%hdfs rm /demo/data/customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a directory recursively (be careful!)\n",
    "%hdfs rm -r /demo/sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1669d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify deletion\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f951b",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "If all cells above executed successfully, webhdfsmagic is working correctly with your HDFS cluster!\n",
    "\n",
    "### Features demonstrated:\n",
    "\n",
    "- ‚úÖ Configuration and connection through Knox Gateway\n",
    "- ‚úÖ Directory listing (`ls`)\n",
    "- ‚úÖ Directory creation (`mkdir`)\n",
    "- ‚úÖ File upload (`put`) with streaming support\n",
    "- ‚úÖ File reading (`cat`) with line limit option\n",
    "- ‚úÖ File download (`get`) with streaming support\n",
    "- ‚úÖ Wildcard support for batch operations\n",
    "- ‚úÖ File deletion (`rm`) with recursive option\n",
    "- ‚úÖ Complete data workflow\n",
    "\n",
    "### Useful URLs:\n",
    "\n",
    "- **HDFS NameNode UI**: http://localhost:9870\n",
    "- **WebHDFS Gateway**: http://localhost:8080/gateway/default/webhdfs/v1/\n",
    "\n",
    "### To stop the environment:\n",
    "\n",
    "```bash\n",
    "docker-compose down\n",
    "# or to also remove data:\n",
    "docker-compose down -v\n",
    "```\n",
    "\n",
    "### Advantages of webhdfsmagic:\n",
    "\n",
    "1. **Simpler syntax**: Magic commands vs Python API calls\n",
    "2. **Less boilerplate**: No client initialization code needed\n",
    "3. **Better integration**: Works naturally in Jupyter notebooks\n",
    "4. **Streaming support**: Efficient for large files\n",
    "5. **Wildcard support**: Batch operations made easy\n",
    "6. **Knox Gateway ready**: Built-in support for enterprise security"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
