{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18ab0b3",
   "metadata": {},
   "source": [
    "# üöÄ webhdfsmagic - Complete Tutorial & Showcase\n",
    "\n",
    "Welcome! This notebook demonstrates **all features** of `webhdfsmagic`, a powerful IPython/Jupyter magic extension for interacting with HDFS clusters via WebHDFS REST API.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "This comprehensive tutorial covers:\n",
    "- ‚úÖ **Setup & Configuration** - Connect to HDFS via Knox Gateway\n",
    "- ‚úÖ **Directory Operations** - Create, list, and navigate directories\n",
    "- ‚úÖ **File Management** - Upload, download, and delete files\n",
    "- ‚úÖ **Smart Preview** - Intelligent data visualization for CSV/TSV/Parquet\n",
    "- ‚úÖ **Permissions** - Manage file permissions (chmod)\n",
    "- ‚úÖ **Advanced Features** - Format options, batch operations, and more\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "1. **Docker environment running**:\n",
    "   ```bash\n",
    "   cd demo && docker-compose up -d\n",
    "   ```\n",
    "   This starts a local HDFS cluster with Knox Gateway for testing.\n",
    "\n",
    "2. **webhdfsmagic installed**:\n",
    "   ```bash\n",
    "   pip install webhdfsmagic\n",
    "   ```\n",
    "\n",
    "3. **Services available**:\n",
    "   - NameNode UI: http://localhost:9870 (HDFS web interface)\n",
    "   - Knox Gateway: http://localhost:8080 (REST API gateway)\n",
    "\n",
    "üí° **Note**: This demo uses a local HDFS cluster, but webhdfsmagic works with any WebHDFS-compatible cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15023650",
   "metadata": {},
   "source": [
    "## Step 1: Load the Extension\n",
    "\n",
    "First, we load the webhdfsmagic extension into Jupyter. This registers the `%hdfs` magic command that we'll use throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77726b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webhdfsmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext webhdfsmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext webhdfsmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe6270",
   "metadata": {},
   "source": [
    "## üìñ Step 2: View Available Commands\n",
    "\n",
    "Let's explore what commands are available. The `help` command shows all webhdfsmagic operations with detailed documentation.\n",
    "\n",
    "**üí° Tip**: If you've just updated webhdfsmagic, restart the kernel and reload this cell to see the latest help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22696670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .hdfs-help {\n",
       "                font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\",\n",
       "                    Helvetica, Arial, sans-serif;\n",
       "            }\n",
       "            .hdfs-help table {\n",
       "                border-collapse: collapse;\n",
       "                width: 100%;\n",
       "                margin: 10px 0;\n",
       "            }\n",
       "            .hdfs-help th {\n",
       "                background-color: #f6f8fa;\n",
       "                padding: 8px;\n",
       "                text-align: left;\n",
       "                border: 1px solid #d0d7de;\n",
       "            }\n",
       "            .hdfs-help td {\n",
       "                padding: 8px;\n",
       "                border: 1px solid #d0d7de;\n",
       "                vertical-align: top;\n",
       "            }\n",
       "            .hdfs-help code {\n",
       "                background-color: #f6f8fa;\n",
       "                padding: 2px 6px;\n",
       "                border-radius: 3px;\n",
       "                font-family: ui-monospace, SFMono-Regular, \"SF Mono\", Menlo, Consolas, monospace;\n",
       "                font-size: 0.9em;\n",
       "            }\n",
       "            .hdfs-help .option {\n",
       "                color: #0969da;\n",
       "                font-weight: 500;\n",
       "            }\n",
       "        </style>\n",
       "        <div class=\"hdfs-help\">\n",
       "        <h3>üìñ WebHDFS Magic Commands</h3>\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th style=\"width: 40%;\">Command</th>\n",
       "                    <th>Description</th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs help</code></td>\n",
       "                    <td>Display this help</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs setconfig {...}</code></td>\n",
       "                    <td>Set configuration (JSON format)</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs ls [path]</code></td>\n",
       "                    <td>List files and directories</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs mkdir &lt;path&gt;</code></td>\n",
       "                    <td>Create directory</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs rm &lt;path&gt; [-r]</code></td>\n",
       "                    <td>Delete file/directory<br>\n",
       "                        <span class=\"option\">-r</span> : recursive deletion</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs put &lt;local&gt; &lt;hdfs&gt;</code></td>\n",
       "                    <td>Upload files (supports wildcards)<br>\n",
       "                        <span class=\"option\">-t, --threads &lt;N&gt;</span> :\n",
       "                        use N parallel threads for multi-file uploads</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs get &lt;hdfs&gt; &lt;local&gt;</code></td>\n",
       "                    <td>Download files (supports wildcards)<br>\n",
       "                        <span class=\"option\">-t, --threads &lt;N&gt;</span> :\n",
       "                        use N parallel threads for multi-file downloads</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs cat &lt;file&gt; [options]</code></td>\n",
       "                    <td><strong>Smart file preview</strong> (CSV/TSV/Parquet)<br>\n",
       "                        <span class=\"option\">-n &lt;lines&gt;</span> :\n",
       "                        limit to N rows (default: 100)<br>\n",
       "                        <span class=\"option\">--format &lt;type&gt;</span> :\n",
       "                        force format (csv, parquet, pandas, polars, raw)<br>\n",
       "                        <span class=\"option\">--raw</span> :\n",
       "                        display raw content without formatting<br>\n",
       "                        <br>\n",
       "                        <strong>Auto-detects:</strong> file format, delimiter,\n",
       "                        data types<br>\n",
       "                        <strong>Formats:</strong>\n",
       "                        <em>pandas</em> (classic),\n",
       "                        <em>polars</em> (with schema),\n",
       "                        <em>grid</em> (default table)</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs chmod [-R] &lt;mode&gt; &lt;path&gt;</code></td>\n",
       "                    <td>Change permissions (e.g., 644, 755)<br>\n",
       "                        <span class=\"option\">-R</span> : recursive</td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td><code>%hdfs chown [-R] &lt;user:group&gt; &lt;path&gt;</code></td>\n",
       "                    <td>Change owner and group<br>\n",
       "                        <span class=\"option\">-R</span> : recursive</td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "        <p><strong>üí° Examples:</strong></p>\n",
       "        <ul>\n",
       "            <li><code>%hdfs cat data.csv -n 10</code> - Preview first 10 rows</li>\n",
       "            <li><code>%hdfs cat data.parquet --format pandas</code> -\n",
       "                Display in pandas format (classic)</li>\n",
       "            <li><code>%hdfs cat data.parquet --format polars</code> -\n",
       "                Display with schema and types</li>\n",
       "            <li><code>%hdfs put *.csv /data/</code> - Upload all CSV files</li>\n",
       "            <li><code>%hdfs put -t 4 ./data/*.csv /hdfs/input/</code> - \n",
       "                Upload files with 4 parallel threads</li>\n",
       "            <li><code>%hdfs get -t 8 /hdfs/output/*.parquet ./results/</code> -\n",
       "                Download files with 8 parallel threads</li>\n",
       "            <li><code>%hdfs chmod -R 755 /mydir</code> - Set permissions recursively</li>\n",
       "        </ul>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2fb5b",
   "metadata": {},
   "source": [
    "## Step 3: Configure HDFS Connection\n",
    "\n",
    "Now we'll create a configuration file that tells webhdfsmagic how to connect to your HDFS cluster.\n",
    "\n",
    "‚ö†Ô∏è **Important**: Knox Gateway requires the `/gateway/default` path in the URL.\n",
    "\n",
    "üìù **What this does**:\n",
    "- Creates `~/.webhdfsmagic/config.json` with connection settings\n",
    "- Specifies Knox Gateway URL, credentials, and SSL settings\n",
    "\n",
    "‚ö†Ô∏è **After running this cell**: You MUST restart the kernel (Kernel ‚Üí Restart) and reload the extension (re-run cells 1-2).\n",
    "\n",
    "**Why?** The extension loads configuration at startup. If you create the config after loading the extension, it will use default (incorrect) settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f2217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration file created successfully!\n",
      "‚ö†Ô∏è  IMPORTANT: Restart the Jupyter kernel now and reload the extension!\n",
      "üìÅ Config saved to: /home/codespace/.webhdfsmagic/config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knox_url': 'http://localhost:8080/gateway/default',\n",
       " 'webhdfs_api': '/webhdfs/v1',\n",
       " 'username': 'hdfs',\n",
       " 'password': 'password',\n",
       " 'verify_ssl': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Create configuration directory\n",
    "config_dir = os.path.expanduser('~/.webhdfsmagic')\n",
    "config_path = os.path.join(config_dir, 'config.json')\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "# Configuration settings\n",
    "config = {\n",
    "    \"knox_url\": \"http://localhost:8080/gateway/default\",  # Knox Gateway endpoint\n",
    "    \"webhdfs_api\": \"/webhdfs/v1\",                         # WebHDFS API path\n",
    "    \"username\": \"hdfs\",                                    # HDFS username\n",
    "    \"password\": \"password\",                                # HDFS password\n",
    "    \"verify_ssl\": False                                    # Disable SSL verification (dev only!)\n",
    "}\n",
    "\n",
    "# Write configuration file\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Configuration file created successfully!\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Restart the Jupyter kernel now and reload the extension!\")\n",
    "print(f\"üìÅ Config saved to: {config_path}\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1a342",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è RESTART KERNEL NOW\n",
    "\n",
    "**Required actions**:\n",
    "1. Click **Kernel ‚Üí Restart** (or the ‚ü≥ button in the toolbar)\n",
    "2. After restart, **re-execute cells 1 and 2** to reload the extension with the correct configuration\n",
    "\n",
    "**Why is this necessary?**\n",
    "\n",
    "The webhdfsmagic extension loads its configuration when `%load_ext webhdfsmagic` is first executed. If you create the config file *after* loading the extension, it will continue using default (incorrect) settings.\n",
    "\n",
    "By restarting the kernel and re-loading the extension, we ensure it picks up the new configuration.\n",
    "\n",
    "‚úÖ **After restarting**: Continue with the next section to start working with HDFS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128bd50",
   "metadata": {},
   "source": [
    "## Step 4: Create Directory Structure\n",
    "\n",
    "```\n",
    "\n",
    "Let's create a demo directory structure in HDFS. We'll use the `mkdir` command to create directories.‚îî‚îÄ‚îÄ /archive   (for temporary/archived files)\n",
    "\n",
    "‚îú‚îÄ‚îÄ /data      (for storing our data files)\n",
    "\n",
    "**Directory structure we're creating**:/demo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a78e2482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Directory /demo created.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the root demo directory\n",
    "%hdfs mkdir /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949d2d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Directory /demo/data created.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a subdirectory for data files\n",
    "%hdfs mkdir /demo/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878cae34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Directory /demo/archive created.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a subdirectory for archives\n",
    "%hdfs mkdir /demo/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3c7d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archive</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-21 15:05:39.680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>DIR</td>\n",
       "      <td>0</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rwxr-xr-x</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-21 15:05:39.647</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name type  size owner       group permissions  block_size  \\\n",
       "0  archive  DIR     0  hdfs  supergroup   rwxr-xr-x           0   \n",
       "1     data  DIR     0  hdfs  supergroup   rwxr-xr-x           0   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-21 15:05:39.680            0  \n",
       "1 2025-12-21 15:05:39.647            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List contents to verify directories were created\n",
    "%hdfs ls /demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716f503",
   "metadata": {},
   "source": [
    "## Step 5: Create Sample Data Files\n",
    "\n",
    "This demonstrates webhdfsmagic's ability to work with multiple data formats.\n",
    "\n",
    "Now let's create sample data files locally to demonstrate webhdfsmagic's file upload and preview capabilities.\n",
    "\n",
    "- **Parquet** (columnar format) - product catalog\n",
    "\n",
    "We'll create three different file formats:- **TSV** (Tab-Separated Values) - customer data  \n",
    "- **CSV** (Comma-Separated Values) - sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d975eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sales.csv created\n",
      "‚úÖ customers.tsv created\n",
      "‚úÖ products.parquet created\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create CSV file: Sales data with dates, products, quantities, and prices\n",
    "sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-01-01', periods=5),\n",
    "    'product': ['Laptop', 'Monitor', 'Keyboard', 'Laptop', 'Monitor'],\n",
    "    'quantity': [1, 2, 3, 1, 1],\n",
    "    'price': [1000.0, 300.0, 80.0, 1000.0, 300.0]\n",
    "})\n",
    "sales.to_csv('sales.csv', index=False)\n",
    "print(\"‚úÖ sales.csv created\")\n",
    "\n",
    "# Create TSV file: Customer data with tab separator\n",
    "customers = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Carol'],\n",
    "    'email': ['alice@example.com', 'bob@example.com', 'carol@example.com']\n",
    "})\n",
    "customers.to_csv('customers.tsv', sep='\\t', index=False)\n",
    "print(\"‚úÖ customers.tsv created\")\n",
    "\n",
    "# Create Parquet file: Product catalog (binary columnar format)\n",
    "products = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Laptop', 'Monitor', 'Keyboard'],\n",
    "    'stock': [50, 120, 500]\n",
    "})\n",
    "products.to_parquet('products.parquet')\n",
    "print(\"‚úÖ products.parquet created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078554d7",
   "metadata": {},
   "source": [
    "## Step 6: Upload Files to HDFS (PUT)\n",
    "\n",
    "This transfers files from your local filesystem to HDFS.\n",
    "\n",
    "Now let's upload our local files to HDFS using the `put` command.\n",
    "\n",
    "**Syntax**: `%hdfs put <local_path> <hdfs_path>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d29d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sales.csv uploaded to /demo/data/sales.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload CSV file\n",
    "%hdfs put sales.csv /demo/data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c3d3d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "customers.tsv uploaded to /demo/data/customers.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload TSV file\n",
    "%hdfs put customers.tsv /demo/data/customers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ef1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "products.parquet uploaded to /demo/data/products.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload Parquet file\n",
    "%hdfs put products.parquet /demo/data/products.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c867bb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customers.tsv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>88</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>products.parquet</td>\n",
       "      <td>FILE</td>\n",
       "      <td>2699</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.682</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>163</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.219</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  type  size owner       group permissions  block_size  \\\n",
       "0     customers.tsv  FILE    88  hdfs  supergroup   rw-r--r--   134217728   \n",
       "1  products.parquet  FILE  2699  hdfs  supergroup   rw-r--r--   134217728   \n",
       "2         sales.csv  FILE   163  hdfs  supergroup   rw-r--r--   134217728   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-21 15:05:40.250            3  \n",
       "1 2025-12-21 15:05:40.682            3  \n",
       "2 2025-12-21 15:05:40.219            3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify files were uploaded successfully\n",
    "%hdfs ls /demo/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e89452",
   "metadata": {},
   "source": [
    "## Step 7: Preview Files with Smart Cat\n",
    "\n",
    "The `cat` command displays file contents with **intelligent format detection**!\n",
    "\n",
    "Let's see it in action!\n",
    "\n",
    "üß† **Smart Cat Features**:\n",
    "\n",
    "- ‚úÖ Auto-detects file format (CSV, TSV, Parquet)\n",
    "- ‚úÖ Auto-detects delimiters (`,` `;` `|` `\\t`)\n",
    "- ‚úÖ Formats output as beautiful tables\n",
    "- ‚úÖ Handles all Parquet data types (int, float, bool, datetime, etc.)\n",
    "- ‚ö° **Ultra-fast Parquet processing** with Polars (3.7x faster than PyArrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201f3723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+---------+\n",
      "| date       | product   |   quantity |   price |\n",
      "+============+===========+============+=========+\n",
      "| 2025-01-01 | Laptop    |          1 |    1000 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-02 | Monitor   |          2 |     300 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-03 | Keyboard  |          3 |      80 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-04 | Laptop    |          1 |    1000 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-05 | Monitor   |          1 |     300 |\n",
      "+------------+-----------+------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# Preview CSV file - automatically formatted as table\n",
    "%hdfs cat /demo/data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3a5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+\n",
      "|   id | name   | email             |\n",
      "+======+========+===================+\n",
      "|    1 | Alice  | alice@example.com |\n",
      "+------+--------+-------------------+\n",
      "|    2 | Bob    | bob@example.com   |\n",
      "+------+--------+-------------------+\n",
      "|    3 | Carol  | carol@example.com |\n",
      "+------+--------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Preview TSV file - tab delimiter auto-detected\n",
    "%hdfs cat /demo/data/customers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4cd4126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n",
      "|   id | name     |   stock |\n",
      "+======+==========+=========+\n",
      "|    1 | Laptop   |      50 |\n",
      "+------+----------+---------+\n",
      "|    2 | Monitor  |     120 |\n",
      "+------+----------+---------+\n",
      "|    3 | Keyboard |     500 |\n",
      "+------+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "# Preview Parquet file - binary format decoded automatically\n",
    "%hdfs cat /demo/data/products.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "937e03f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+---------+\n",
      "| date       | product   |   quantity |   price |\n",
      "+============+===========+============+=========+\n",
      "| 2025-01-01 | Laptop    |          1 |    1000 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-02 | Monitor   |          2 |     300 |\n",
      "+------------+-----------+------------+---------+\n",
      "\n",
      "... (showing first 2 of 6 rows)\n"
     ]
    }
   ],
   "source": [
    "# Preview with limit: show only first 2 rows\n",
    "%hdfs cat -n 2 /demo/data/sales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f485125c",
   "metadata": {},
   "source": [
    "### üéØ Advanced CAT Options\n",
    "\n",
    "Let's explore more `cat` command features:\n",
    "- Custom row limits with `-n`\n",
    "- Format comparisons\n",
    "- Delimiter detection demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d1d2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ First 5 rows of CSV file:\n",
      "+------------+-----------+------------+---------+\n",
      "| date       | product   |   quantity |   price |\n",
      "+============+===========+============+=========+\n",
      "| 2025-01-01 | Laptop    |          1 |    1000 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-02 | Monitor   |          2 |     300 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-03 | Keyboard  |          3 |      80 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-04 | Laptop    |          1 |    1000 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-05 | Monitor   |          1 |     300 |\n",
      "+------------+-----------+------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# Custom preview: Show first 5 rows instead of default\n",
    "print(\"üìÑ First 5 rows of CSV file:\")\n",
    "%hdfs cat -n 5 /demo/data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a24f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Parquet format (optimized read):\n",
      "+------+----------+---------+\n",
      "|   id | name     |   stock |\n",
      "+======+==========+=========+\n",
      "|    1 | Laptop   |      50 |\n",
      "+------+----------+---------+\n",
      "|    2 | Monitor  |     120 |\n",
      "+------+----------+---------+\n",
      "|    3 | Keyboard |     500 |\n",
      "+------+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "# Parquet format (optimized columnar storage)\n",
    "print(\"üìä Parquet format (optimized read):\")\n",
    "%hdfs cat /demo/data/products.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d83d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã TSV format (tab delimiter auto-detected):\n",
      "+------+--------+-------------------+\n",
      "|   id | name   | email             |\n",
      "+======+========+===================+\n",
      "|    1 | Alice  | alice@example.com |\n",
      "+------+--------+-------------------+\n",
      "|    2 | Bob    | bob@example.com   |\n",
      "+------+--------+-------------------+\n",
      "|    3 | Carol  | carol@example.com |\n",
      "+------+--------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# TSV format - tab delimiter automatically detected\n",
    "print(\"üìã TSV format (tab delimiter auto-detected):\")\n",
    "%hdfs cat /demo/data/customers.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3aeae",
   "metadata": {},
   "source": [
    "### üêº Format Option: --format pandas\n",
    "\n",
    "**What's the difference?**\n",
    "\n",
    "- Familiar format for pandas users\n",
    "\n",
    "By default, `cat` displays data in a formatted grid (using tabulate). With `--format pandas`, you get the standard pandas DataFrame text representation instead.- Simpler, more compact output\n",
    "\n",
    "- Copying/pasting data into terminals or text reports\n",
    "**When to use it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c408263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä GRID format (default):\n",
      "+------------+-----------+------------+---------+\n",
      "| date       | product   |   quantity |   price |\n",
      "+============+===========+============+=========+\n",
      "| 2025-01-01 | Laptop    |          1 |    1000 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-02 | Monitor   |          2 |     300 |\n",
      "+------------+-----------+------------+---------+\n",
      "| 2025-01-03 | Keyboard  |          3 |      80 |\n",
      "+------------+-----------+------------+---------+\n",
      "\n",
      "... (showing first 3 of 6 rows)\n"
     ]
    }
   ],
   "source": [
    "# Default format: GRID (tabulate)\n",
    "print(\"üìä GRID format (default):\")\n",
    "%hdfs cat -n 3 /demo/data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf5b30d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº PANDAS format (--format pandas):\n",
      "         date   product  quantity   price\n",
      "0  2025-01-01    Laptop         1  1000.0\n",
      "1  2025-01-02   Monitor         2   300.0\n",
      "2  2025-01-03  Keyboard         3    80.0\n"
     ]
    }
   ],
   "source": [
    "# Pandas format: DataFrame style\n",
    "print(\"üêº PANDAS format (--format pandas):\")\n",
    "%hdfs cat -n 3 /demo/data/sales.csv --format pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18838704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº PANDAS format with Parquet:\n",
      "   id      name  stock\n",
      "0   1    Laptop     50\n",
      "1   2   Monitor    120\n",
      "2   3  Keyboard    500\n"
     ]
    }
   ],
   "source": [
    "# Works with Parquet too!\n",
    "print(\"üêº PANDAS format with Parquet:\")\n",
    "%hdfs cat -n 3 /demo/data/products.parquet --format pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb5962",
   "metadata": {},
   "source": [
    "üí° **Pro tip**: Use `--format pandas` when you need plain text output for copying into reports, emails, or terminals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb513e1",
   "metadata": {},
   "source": [
    "### ‚ö° Format Option: --format polars\n",
    "\n",
    "**What's Polars?**\n",
    "\n",
    "Polars is a **blazingly fast** DataFrame library written in Rust. When you use `--format polars`, you get:\n",
    "- **3.7x faster** processing than pandas for Parquet files\n",
    "- **Explicit data types** (str, i64, f64, bool) shown in the preview\n",
    "- **Schema information** for better data validation\n",
    "- **Memory efficient** - uses lazy evaluation where possible\n",
    "\n",
    "**When to use it?**\n",
    "- When you need to validate data types in large files\n",
    "- For performance-critical workflows with Parquet files\n",
    "- When you want to see the exact schema at a glance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e64686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° POLARS format (--format polars) - CSV with schema:\n",
      "shape: (3, 4)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ date       ‚îÜ product  ‚îÜ quantity ‚îÜ price  ‚îÇ\n",
      "‚îÇ ---        ‚îÜ ---      ‚îÜ ---      ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str        ‚îÜ str      ‚îÜ i64      ‚îÜ f64    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 2025-01-01 ‚îÜ Laptop   ‚îÜ 1        ‚îÜ 1000.0 ‚îÇ\n",
      "‚îÇ 2025-01-02 ‚îÜ Monitor  ‚îÜ 2        ‚îÜ 300.0  ‚îÇ\n",
      "‚îÇ 2025-01-03 ‚îÜ Keyboard ‚îÜ 3        ‚îÜ 80.0   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "... (showing first 3 of 6 rows)\n"
     ]
    }
   ],
   "source": [
    "# Polars format: Shows schema with explicit types\n",
    "print(\"‚ö° POLARS format (--format polars) - CSV with schema:\")\n",
    "%hdfs cat -n 3 /demo/data/sales.csv --format polars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "967d0481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° POLARS format (--format polars) - Parquet (3.7x faster!):\n",
      "shape: (3, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ id  ‚îÜ name     ‚îÜ stock ‚îÇ\n",
      "‚îÇ --- ‚îÜ ---      ‚îÜ ---   ‚îÇ\n",
      "‚îÇ i64 ‚îÜ str      ‚îÜ i64   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1   ‚îÜ Laptop   ‚îÜ 50    ‚îÇ\n",
      "‚îÇ 2   ‚îÜ Monitor  ‚îÜ 120   ‚îÇ\n",
      "‚îÇ 3   ‚îÜ Keyboard ‚îÜ 500   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "# Polars format with Parquet - much faster than pandas!\n",
    "print(\"‚ö° POLARS format (--format polars) - Parquet (3.7x faster!):\")\n",
    "%hdfs cat -n 3 /demo/data/products.parquet --format polars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21639553",
   "metadata": {},
   "source": [
    "### üìù Format Option: --raw\n",
    "\n",
    "**Raw format** displays the **unformatted, raw content** of the file - exactly as it is stored.\n",
    "\n",
    "- No table formatting, no DataFrames\n",
    "- Shows the original file content (CSV text, Parquet header bytes, etc.)\n",
    "- Useful for debugging or when you want to see the file \"as-is\"\n",
    "\n",
    "**When to use it?**\n",
    "- Debugging file encoding or format issues\n",
    "- Examining raw file content for inspection\n",
    "- When you need the exact bytes/text without any processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aead12cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù RAW format (--raw) - First 5 lines of CSV:\n",
      "date,product,quantity,price\n",
      "2025-01-01,Laptop,1,1000.0\n",
      "2025-01-02,Monitor,2,300.0\n",
      "2025-01-03,Keyboard,3,80.0\n",
      "2025-01-04,Laptop,1,1000.0\n"
     ]
    }
   ],
   "source": [
    "# Raw format: Unformatted content\n",
    "print(\"üìù RAW format (--raw) - First 5 lines of CSV:\")\n",
    "%hdfs cat -n 5 /demo/data/sales.csv --raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7417b1",
   "metadata": {},
   "source": [
    "## Step 8: Manage Permissions (CHMOD)\n",
    "\n",
    "Just like Unix/Linux, HDFS has file permissions! Use `chmod` to control access.\n",
    "\n",
    "**Permission format**: `chmod <mode> <path>`\n",
    "- **644**: Read/write for owner, read-only for others\n",
    "- **755**: Read/write/execute for owner, read/execute for others\n",
    "- **-R**: Recursive (applies to all files in directory)\n",
    "\n",
    "Let's see current permissions, then modify them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbe5d1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customers.tsv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>88</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>products.parquet</td>\n",
       "      <td>FILE</td>\n",
       "      <td>2699</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.682</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>163</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.219</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  type  size owner       group permissions  block_size  \\\n",
       "0     customers.tsv  FILE    88  hdfs  supergroup   rw-r--r--   134217728   \n",
       "1  products.parquet  FILE  2699  hdfs  supergroup   rw-r--r--   134217728   \n",
       "2         sales.csv  FILE   163  hdfs  supergroup   rw-r--r--   134217728   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-21 15:05:40.250            3  \n",
       "1 2025-12-21 15:05:40.682            3  \n",
       "2 2025-12-21 15:05:40.219            3  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs ls /demo/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dc4911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Permission 644 set for /demo/data/sales.csv'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs chmod 644 /demo/data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48425539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Permission 644 set for /demo/data/customers.tsv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs chmod 644 /demo/data/customers.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b68ff6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Permission 644 set for /demo/data/products.parquet'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs chmod 644 /demo/data/products.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da3fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recursive chmod 755 applied on /demo/archive'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs chmod -R 755 /demo/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f156d407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customers.tsv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>88</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>products.parquet</td>\n",
       "      <td>FILE</td>\n",
       "      <td>2699</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.682</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>163</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:40.219</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  type  size owner       group permissions  block_size  \\\n",
       "0     customers.tsv  FILE    88  hdfs  supergroup   rw-r--r--   134217728   \n",
       "1  products.parquet  FILE  2699  hdfs  supergroup   rw-r--r--   134217728   \n",
       "2         sales.csv  FILE   163  hdfs  supergroup   rw-r--r--   134217728   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-21 15:05:40.250            3  \n",
       "1 2025-12-21 15:05:40.682            3  \n",
       "2 2025-12-21 15:05:40.219            3  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs ls /demo/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb32a40",
   "metadata": {},
   "source": [
    "## Step 9: Download Files from HDFS (GET)\n",
    "\n",
    "Need to download files from HDFS to your local machine? Use the `get` command!\n",
    "\n",
    "**Syntax**: `%hdfs get <hdfs_path> <local_path>`\n",
    "\n",
    "This is the reverse of `put` - it transfers files from HDFS to your local filesystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21f7586b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/demo/data/sales.csv downloaded to ./local_sales.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs get /demo/data/sales.csv ./local_sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cc4a1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File downloaded successfully: 5 rows\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./local_sales.csv'):\n",
    "    df = pd.read_csv('./local_sales.csv')\n",
    "    print(f\"‚úÖ File downloaded successfully: {len(df)} rows\")\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abe3b0",
   "metadata": {},
   "source": [
    "## Step 10: Delete Files and Directories (RM)\n",
    "\n",
    "Clean up your HDFS workspace with the `rm` command.\n",
    "\n",
    "**‚ö†Ô∏è Warning**: Deletion is **permanent**! There's no recycle bin in HDFS.\n",
    "\n",
    "**Options**:\n",
    "- `-r` or `-R`: Recursive (required for non-empty directories)\n",
    "- `-skipTrash`: Bypass trash (immediate permanent deletion)\n",
    "\n",
    "Let's create temporary files, then delete them to demonstrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28234b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary files locally\n",
    "for i in range(1, 3):\n",
    "    temp = pd.DataFrame({'id': [i], 'value': [f'temp_{i}']})\n",
    "    temp.to_csv(f'temp_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57b45909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "temp_1.csv uploaded to /demo/archive/\n",
      "temp_2.csv uploaded to /demo/archive/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%hdfs put temp_*.csv /demo/archive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e631a1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>owner</th>\n",
       "      <th>group</th>\n",
       "      <th>permissions</th>\n",
       "      <th>block_size</th>\n",
       "      <th>modified</th>\n",
       "      <th>replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>temp_1.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>18</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:41.669</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temp_2.csv</td>\n",
       "      <td>FILE</td>\n",
       "      <td>18</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>supergroup</td>\n",
       "      <td>rw-r--r--</td>\n",
       "      <td>134217728</td>\n",
       "      <td>2025-12-21 15:05:42.083</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  type  size owner       group permissions  block_size  \\\n",
       "0  temp_1.csv  FILE    18  hdfs  supergroup   rw-r--r--   134217728   \n",
       "1  temp_2.csv  FILE    18  hdfs  supergroup   rw-r--r--   134217728   \n",
       "\n",
       "                 modified  replication  \n",
       "0 2025-12-21 15:05:41.669            3  \n",
       "1 2025-12-21 15:05:42.083            3  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs ls /demo/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b421a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/demo/archive/temp_1.csv deleted'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs rm /demo/archive/temp_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb767727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/demo/archive/temp_2.csv deleted'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs rm /demo/archive/temp_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed19bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'empty_dir': True, 'path': '/demo/archive'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs ls /demo/archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b40ab5",
   "metadata": {},
   "source": [
    "## Step 11: Advanced Parquet Features\n",
    "\n",
    "Let's test webhdfsmagic's ability to handle complex Parquet files with multiple data types.\n",
    "\n",
    "This demonstrates Smart Cat's sophisticated type handling!\n",
    "\n",
    "We'll create a Parquet file with:\n",
    "\n",
    "- **Integers** (id)- **Timestamps** (datetime)\n",
    "\n",
    "- **Strings** (name)- **Categories** (category)\n",
    "\n",
    "- **Floats** (score)- **Booleans** (active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c80b1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complex Parquet file created with multiple data types!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>active</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>User_1</td>\n",
       "      <td>91.52</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>User_2</td>\n",
       "      <td>9.07</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>2025-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>User_3</td>\n",
       "      <td>1.43</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>2025-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>User_4</td>\n",
       "      <td>23.72</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>User_5</td>\n",
       "      <td>94.83</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>User_6</td>\n",
       "      <td>43.15</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>2025-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>User_7</td>\n",
       "      <td>77.44</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>User_8</td>\n",
       "      <td>47.48</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>2025-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>User_9</td>\n",
       "      <td>12.10</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>User_10</td>\n",
       "      <td>14.69</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>2025-01-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     name  score  active category  timestamp\n",
       "0   1   User_1  91.52   False        B 2025-01-01\n",
       "1   2   User_2   9.07   False        A 2025-01-02\n",
       "2   3   User_3   1.43   False        C 2025-01-03\n",
       "3   4   User_4  23.72   False        B 2025-01-04\n",
       "4   5   User_5  94.83    True        B 2025-01-05\n",
       "5   6   User_6  43.15   False        A 2025-01-06\n",
       "6   7   User_7  77.44   False        B 2025-01-07\n",
       "7   8   User_8  47.48    True        C 2025-01-08\n",
       "8   9   User_9  12.10   False        B 2025-01-09\n",
       "9  10  User_10  14.69   False        B 2025-01-10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create complex Parquet file with multiple data types\n",
    "import numpy as np\n",
    "\n",
    "complex_data = pd.DataFrame({\n",
    "    'id': range(1, 11),                                      # Integer\n",
    "    'name': [f'User_{i}' for i in range(1, 11)],           # String\n",
    "    'score': np.random.uniform(0, 100, 10).round(2),       # Float\n",
    "    'active': np.random.choice([True, False], 10),         # Boolean\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10),     # Category\n",
    "    'timestamp': pd.date_range('2025-01-01', periods=10, freq='D')  # Datetime\n",
    "})\n",
    "\n",
    "complex_data.to_parquet('complex_data.parquet')\n",
    "print(\"‚úÖ Complex Parquet file created with multiple data types!\")\n",
    "complex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40feb686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "complex_data.parquet uploaded to /demo/data/complex_data.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload to HDFS\n",
    "%hdfs put complex_data.parquet /demo/data/complex_data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a723e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Preview complex Parquet file:\n",
      "   (types: int, str, float, bool, category, datetime)\n",
      "\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|   id | name    |   score | active   | category   | timestamp           |\n",
      "+======+=========+=========+==========+============+=====================+\n",
      "|    1 | User_1  |   91.52 | False    | B          | 2025-01-01 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    2 | User_2  |    9.07 | False    | A          | 2025-01-02 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    3 | User_3  |    1.43 | False    | C          | 2025-01-03 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    4 | User_4  |   23.72 | False    | B          | 2025-01-04 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    5 | User_5  |   94.83 | True     | B          | 2025-01-05 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    6 | User_6  |   43.15 | False    | A          | 2025-01-06 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    7 | User_7  |   77.44 | False    | B          | 2025-01-07 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    8 | User_8  |   47.48 | True     | C          | 2025-01-08 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|    9 | User_9  |   12.1  | False    | B          | 2025-01-09 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n",
      "|   10 | User_10 |   14.69 | False    | B          | 2025-01-10 00:00:00 |\n",
      "+------+---------+---------+----------+------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Smart Cat handles all column types automatically!\n",
    "print(\"üìä Preview complex Parquet file:\")\n",
    "print(\"   (types: int, str, float, bool, category, datetime)\\n\")\n",
    "%hdfs cat /demo/data/complex_data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2bcbb78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preview first 3 rows:\n",
      "+------+--------+---------+----------+------------+---------------------+\n",
      "|   id | name   |   score | active   | category   | timestamp           |\n",
      "+======+========+=========+==========+============+=====================+\n",
      "|    1 | User_1 |   91.52 | False    | B          | 2025-01-01 00:00:00 |\n",
      "+------+--------+---------+----------+------------+---------------------+\n",
      "|    2 | User_2 |    9.07 | False    | A          | 2025-01-02 00:00:00 |\n",
      "+------+--------+---------+----------+------------+---------------------+\n",
      "|    3 | User_3 |    1.43 | False    | C          | 2025-01-03 00:00:00 |\n",
      "+------+--------+---------+----------+------------+---------------------+\n",
      "\n",
      "... (showing first 3 of 3 rows)\n"
     ]
    }
   ],
   "source": [
    "# Preview first 3 rows only\n",
    "print(\"üìÑ Preview first 3 rows:\")\n",
    "%hdfs cat -n 3 /demo/data/complex_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ae0a1",
   "metadata": {},
   "source": [
    "## Error Handling: Directory Not Found\n",
    "\n",
    "The updated webhdfsmagic now provides **user-friendly error messages** when accessing non-existent directories.\n",
    "\n",
    "Instead of showing a long HTTP 404 traceback, it simply displays:\n",
    "```\n",
    "Directory not found: <path>\n",
    "```\n",
    "\n",
    "This makes it much easier to understand what went wrong!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4792bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: ERROR in GET LISTSTATUS: HTTPError: 404 Client Error: Not Found for url: http://localhost:8080/gateway/default/webhdfs/v1/nonexistent_path?op=LISTSTATUS&user.name=hdfs\n",
      "ERROR:     url: http://localhost:8080/gateway/default/webhdfs/v1/nonexistent_path\n",
      "ERROR:     status_code: None\n",
      "ERROR:     response_text: None\n",
      "ERROR: Full traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/webhdfsmagic/webhdfsmagic/client.py\", line 112, in execute\n",
      "    response.raise_for_status()\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://localhost:8080/gateway/default/webhdfs/v1/nonexistent_path?op=LISTSTATUS&user.name=hdfs\n",
      "ERROR: ERROR in GET LISTSTATUS: HTTPError: 404 Client Error: Not Found for url: http://localhost:8080/gateway/default/webhdfs/v1/user/notfound/data?op=LISTSTATUS&user.name=hdfs\n",
      "ERROR:     url: http://localhost:8080/gateway/default/webhdfs/v1/user/notfound/data\n",
      "ERROR:     status_code: None\n",
      "ERROR:     response_text: None\n",
      "ERROR: Full traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/webhdfsmagic/webhdfsmagic/client.py\", line 112, in execute\n",
      "    response.raise_for_status()\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://localhost:8080/gateway/default/webhdfs/v1/user/notfound/data?op=LISTSTATUS&user.name=hdfs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Attempting to list a non-existent directory...\n",
      "Result: Directory not found: /nonexistent_path\n",
      "\n",
      "Test 2: Attempting to list another non-existent directory...\n",
      "Result: Directory not found: /user/notfound/data\n"
     ]
    }
   ],
   "source": [
    "# Test 1: List a non-existent directory\n",
    "print(\"Test 1: Attempting to list a non-existent directory...\")\n",
    "result = %hdfs ls /nonexistent_path\n",
    "print(f\"Result: {result}\")\n",
    "print()\n",
    "\n",
    "# Test 2: List another non-existent path\n",
    "print(\"Test 2: Attempting to list another non-existent directory...\")\n",
    "result2 = %hdfs ls /user/notfound/data\n",
    "print(f\"Result: {result2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0094a7",
   "metadata": {},
   "source": [
    "## üöÄ Step 12: Parallel Uploads & Downloads (Multi-threaded PUT/GET)\n",
    "\n",
    "Starting from version 0.0.4, webhdfsmagic supports **parallel file transfers** using the `--threads` (or `-t`) option for both `put` and `get` commands.\n",
    "\n",
    "This allows you to upload or download multiple files simultaneously, greatly speeding up operations on large datasets or many files.\n",
    "\n",
    "**Key features:**\n",
    "- Multi-threaded transfers for PUT and GET\n",
    "- Syntax: `%hdfs put --threads N <local_files> <hdfs_dir>`\n",
    "- Syntax: `%hdfs get --threads N <hdfs_files> <local_dir>`\n",
    "- N = number of threads (e.g. 4, 8, 16)\n",
    "\n",
    "Below, we demonstrate parallel upload and download with example commands and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "843aa0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "downloaded_customers.csv uploaded to /demo/data/\n",
      "old_data_1.csv uploaded to /demo/data/\n",
      "customers.csv uploaded to /demo/data/\n",
      "old_data_2.csv uploaded to /demo/data/\n",
      "test_file_4.csv uploaded to /demo/data/\n",
      "local_sales.csv uploaded to /demo/data/\n",
      "sales.csv uploaded to /demo/data/\n",
      "test_file_1.csv uploaded to /demo/data/\n",
      "sales_20251220.csv uploaded to /demo/data/\n",
      "sales_20251221.csv uploaded to /demo/data/\n",
      "sales_20251219.csv uploaded to /demo/data/\n",
      "test_file_3.csv uploaded to /demo/data/\n",
      "old_data_3.csv uploaded to /demo/data/\n",
      "test_file_5.csv uploaded to /demo/data/\n",
      "test_file_2.csv uploaded to /demo/data/\n",
      "temp_1.csv uploaded to /demo/data/\n",
      "temp_2.csv uploaded to /demo/data/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parallel upload: PUT multiple files to HDFS using 4 threads\n",
    "# This will upload all CSV files in the current directory to /demo/data in parallel\n",
    "%hdfs put --threads 4 *.csv /demo/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea3e9466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "complex_data.parquet downloaded to downloads/complex_data.parquet\n",
      "customers.tsv downloaded to downloads/customers.tsv\n",
      "downloaded_customers.csv downloaded to downloads/downloaded_customers.csv\n",
      "customers.csv downloaded to downloads/customers.csv\n",
      "old_data_2.csv downloaded to downloads/old_data_2.csv\n",
      "old_data_3.csv downloaded to downloads/old_data_3.csv\n",
      "old_data_1.csv downloaded to downloads/old_data_1.csv\n",
      "local_sales.csv downloaded to downloads/local_sales.csv\n",
      "sales_20251219.csv downloaded to downloads/sales_20251219.csv\n",
      "products.parquet downloaded to downloads/products.parquet\n",
      "sales_20251220.csv downloaded to downloads/sales_20251220.csv\n",
      "sales.csv downloaded to downloads/sales.csv\n",
      "sales_20251221.csv downloaded to downloads/sales_20251221.csv\n",
      "temp_2.csv downloaded to downloads/temp_2.csv\n",
      "temp_1.csv downloaded to downloads/temp_1.csv\n",
      "test_file_2.csv downloaded to downloads/test_file_2.csv\n",
      "test_file_1.csv downloaded to downloads/test_file_1.csv\n",
      "test_file_3.csv downloaded to downloads/test_file_3.csv\n",
      "test_file_5.csv downloaded to downloads/test_file_5.csv\n",
      "test_file_4.csv downloaded to downloads/test_file_4.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parallel download: GET multiple files from HDFS using 4 threads\n",
    "# This will download all files from /demo/data to the local ./downloads directory in parallel\n",
    "%hdfs get --threads 4 /demo/data/* ./downloads/\n",
    "\n",
    "# You can also use the short option -t\n",
    "# Example: %hdfs put -t 8 *.tsv /demo/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db1314",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This will permanently delete all demo files and directories!\n",
    "\n",
    "Uncomment and run the cell below to clean up the demo workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "513ff6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/demo deleted'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs rm -R /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "693d87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: ERROR in GET LISTSTATUS: HTTPError: 404 Client Error: Not Found for url: http://localhost:8080/gateway/default/webhdfs/v1/demo?op=LISTSTATUS&user.name=hdfs\n",
      "ERROR:     url: http://localhost:8080/gateway/default/webhdfs/v1/demo\n",
      "ERROR:     status_code: None\n",
      "ERROR:     response_text: None\n",
      "ERROR: Full traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/webhdfsmagic/webhdfsmagic/client.py\", line 112, in execute\n",
      "    response.raise_for_status()\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://localhost:8080/gateway/default/webhdfs/v1/demo?op=LISTSTATUS&user.name=hdfs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Directory not found: /demo'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%hdfs ls  /demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
